{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3d543d",
   "metadata": {},
   "source": [
    "<h1><center> Python project proposal </center></h1>\n",
    "<h2><center>Current car prices from bazos.cz</center></h2>\n",
    "<h3><center>Daniel Brosko, Vojtěch Suchánek</center></h3>\n",
    "\n",
    "Our goal is to web-scrape advertisements listed on website bazos.cz, which is currently one of the most used websites for selling used cars in Czech republic. It has more than 15 000 car adds daily. On the other hand, it has really poor search options, which pretty much complicates searching for desired car based on your parameters.\n",
    "\n",
    "We are going to code algorithm, which will scan adds for the current day, pick those, which fulfill our conditions on date and car type and save their links. Then we will go to each link and save the text of the add. Then we will try to analyze the text of the add to find our parameters. In the end, we will perform some visualizations of distribution of price and selected parameters and try to select some underprices offers.\n",
    "\n",
    "This task might be better to perform continuouslly, for example each hour, to not overload the website. This approach might also allow for longer time period to analyze the data. But since this project should be designed as one-time run, we decided to limit the data to only current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d614ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "988784bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = requests.get('https://auto.bazos.cz/robots.txt')\n",
    "#print(bots.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887d0ee",
   "metadata": {},
   "source": [
    "From the robots page we can see that our actions done in our projects are allowed, since we are not gonna use these search commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d424b1",
   "metadata": {},
   "source": [
    "Here we will get the main page from car section of bazos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f9eff4be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [205]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m no_of_adv_html \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m get_no_adv \u001b[38;5;241m=\u001b[39m no_of_adv_html\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistainzerat inzeratyflex\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 12\u001b[0m get_no_adv \u001b[38;5;241m=\u001b[39m \u001b[43mget_no_adv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minzeratynadpis\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     13\u001b[0m get_no_adv \u001b[38;5;241m=\u001b[39m get_no_adv\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     14\u001b[0m get_no_adv \u001b[38;5;241m=\u001b[39m get_no_adv\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# firstly, we define the input variable so the user can search according to their preference\n",
    "user_search_input = str(\"octávia 3\")\n",
    "user_search_input = re.sub(r\"[^\\w\\s]\", '', user_search_input)\n",
    "user_search_input = re.sub(r\"\\s+\", '+', user_search_input)\n",
    "\n",
    "no_of_adv_url = 'https://auto.bazos.cz/0/?hledat=' + user_search_input + '&hlokalita=&humkreis=25&cenaod=&cenado=&order='\n",
    "\n",
    "page = requests.get(no_of_adv_url)\n",
    "no_of_adv_html = BeautifulSoup(page.text, 'html')\n",
    "\n",
    "get_no_adv = no_of_adv_html.find('div', {'class':'listainzerat inzeratyflex'})\n",
    "get_no_adv = get_no_adv.find('div', {'class':'inzeratynadpis'})\n",
    "get_no_adv = get_no_adv.text\n",
    "get_no_adv = get_no_adv.split(\"z \")[-1]\n",
    "number_adv = get_no_adv.replace(\" \",\"\")\n",
    "number_adv = int(number_adv)\n",
    "# works properly, \"number_adv\" represents total number of advertisements for particular search input,\n",
    "# however, we modify it so that it correspondents correctly to the structure of page\n",
    "no_of_ad_lists = number_adv // 20\n",
    "if number_adv % 20 == 0:\n",
    "    number_adv = no_of_ad_lists * 19\n",
    "else:\n",
    "    number_adv = no_of_ad_lists * 20\n",
    "\n",
    "\n",
    "number_sequence = range(0, number_adv, 20) #start, stop (not included), step\n",
    "# we create empty list for saving the urls,\n",
    "# and then append other tabs with offers (since there are only 20 offers per tab by default)\n",
    "main_url_list = list()\n",
    "\n",
    "for i in range(0,len(number_sequence)):\n",
    "    url = f'https://auto.bazos.cz/{number_sequence[i]}/'\n",
    "    url = url + '?hledat=' + user_search_input + '&hlokalita=&humkreis=25&cenaod=&cenado=&order='\n",
    "    main_url_list.append(url)\n",
    "# here we can check the list of urls for particular tabs\n",
    "print(main_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "878c4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the next step, we get the text of each of those tabs using the BeautifulSoup function,\n",
    "# and save it as elements of the \"soup_list\"\n",
    "soup_list = list()\n",
    "for url in main_url_list:\n",
    "    page = requests.get(url)\n",
    "    soup_list.append(BeautifulSoup(page.text, 'html'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839ec54",
   "metadata": {},
   "source": [
    "There are 20 adds plus other page elements. Lets pick just one add to see their construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889ec75",
   "metadata": {},
   "source": [
    "There are several elements, which are important to us. In this section, we are interested in add id, which we are able to extract from href in element a on the second line. Its the number after \"inzerat/\". We are also interested in the href itself, since it is our link to follow to the actual add. Another element important to us is h2 class = \"nadpis\", which contains title of the add, from which we can extract type of car. And the last interesting element is span with class \"velikost10\", which reffers to date uploaded.\n",
    "\n",
    "Now we can follow the link to the actual add page to see its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f460d7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [201]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m soup_list:\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minzeraty inzeratyflex\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m----> 8\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minzeratynadpis\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      9\u001b[0m     attribute_a \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     w \u001b[38;5;241m=\u001b[39m attribute_a\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "# by the following code, we get the urls of each advertisement /offer/ (listed in the tabs we work with),\n",
    "# and save it to \"list_of_offers_url\"\n",
    "list_of_offers_url = list()\n",
    "\n",
    "for element in soup_list:\n",
    "    x = element.find('div', {'class':'inzeraty inzeratyflex'})\n",
    "    y = x.find('div', {'class':'inzeratynadpis'})\n",
    "    attribute_a = y.find('a')\n",
    "    w = attribute_a.get('href')\n",
    "    \n",
    "    attribute_span = y.find('span')\n",
    "    \n",
    "    #here we just obtain the date from the particular advertisement to make sure we analyze only ads added today\n",
    "    date_str = str(attribute_span.text)\n",
    "    date_str = date_str.replace(\" \",\"\")\n",
    "    date_str = date_str.strip(\"-[]\")\n",
    "    date_str = date_str.strip(\"TOP-[]\")\n",
    "    date_object = datetime.strptime(date_str,'%d.%m.%Y')\n",
    "    date_object = date_object.date()\n",
    "    \n",
    "    if date_object != today:\n",
    "        continue\n",
    "    \n",
    "    list_of_offers_url.append(f'https://auto.bazos.cz{w}')\n",
    "\n",
    "    \n",
    "print(list_of_offers_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "de23b33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://auto.bazos.cz/inzerat/157012769/vw-golf-7-20tdi-110kw-107tkm-plna-zaruka-2-roky-zdarma.php',\n",
       "  '2018',\n",
       "  '107604'],\n",
       " ['https://auto.bazos.cz/inzerat/156954743/skoda-octavia-3dsg-20-tdi.php',\n",
       "  '2013',\n",
       "  '207000']]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_spaces(text):\n",
    "    return text.replace(\" \", \"\")\n",
    "def get_numbers_from_text(text):\n",
    "    import re\n",
    "    text = remove_spaces(text)\n",
    "    pattern = '[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?'\n",
    "    list_of_numbers = re.findall(pattern, text)\n",
    "    return list_of_numbers\n",
    "def find_years(numbers):\n",
    "    numbers = [x for x in numbers if (float(x) > 1980) and (float(x) < 2030)]\n",
    "    return numbers\n",
    "def find_km(numbers):\n",
    "    numbers = [x for x in numbers if (float(x) > 3000) and (float(x) < 500000)]\n",
    "    return numbers\n",
    "def get_context(text, list_of_tokens, year_dictionary = ['egistr', 'r.v', 'vyrob', 'výrob', 'prov'], km_dictionary = ['km', 'Km', 'KM', 'ilomet', 'ajet', 'ájez', 'achom', 'atoč'], context_span=20):\n",
    "    import re\n",
    "    context = []\n",
    "    year = 'No match'\n",
    "    km = 'No match'\n",
    "    for token in find_years(list_of_tokens):\n",
    "        all_occurences_indices = [m.start() for m in re.finditer(token, text)]\n",
    "        for index in all_occurences_indices:\n",
    "            left_index = max(index - context_span, 0)\n",
    "            right_index = min(index + context_span, len(text))\n",
    "            substring = text[left_index: right_index].strip()\n",
    "            for s in year_dictionary:\n",
    "                year_find = [m.start() for m in re.finditer(s, substring)]\n",
    "                if len(year_find) > 0:\n",
    "                    year = token\n",
    "    for token in find_km(list_of_tokens):\n",
    "        all_occurences_indices = [m.start() for m in re.finditer(token, text)]\n",
    "        for index in all_occurences_indices:\n",
    "            left_index = max(index - context_span, 0)\n",
    "            right_index = min(index + context_span, len(text))\n",
    "            substring = text[left_index: right_index].strip()\n",
    "            for s in km_dictionary:\n",
    "                km_find = [m.start() for m in re.finditer(s, substring)]\n",
    "                if len(km_find) > 0:\n",
    "                    km = token\n",
    "    return [year, km]\n",
    "def get_info(links):\n",
    "    results = []\n",
    "    for i in links:\n",
    "        add_page = requests.get(i)\n",
    "        soup_add = BeautifulSoup(add_page.text, 'html')\n",
    "        add = remove_spaces(soup_add.find('div', {'class':'popisdetail'}).get_text())\n",
    "        all_numbers = get_numbers_from_text(add)\n",
    "        context_got = get_context(add, all_numbers)\n",
    "        result = [i, context_got[0], context_got[1]]\n",
    "        results.append(result)\n",
    "    return results\n",
    "link = ['https://auto.bazos.cz/inzerat/157012769/vw-golf-7-20tdi-110kw-107tkm-plna-zaruka-2-roky-zdarma.php', 'https://auto.bazos.cz/inzerat/156954743/skoda-octavia-3dsg-20-tdi.php']\n",
    "#link = list_of_offers_url\n",
    "get_info(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93548d11",
   "metadata": {},
   "source": [
    "Here we can see the description, which we will use to extract parameters of the car. This will be one of the most difficult parts of the project to extract the relevant text in unformated text. There is no official format to the text, so we will have to find a way how to extract this information from various formats.\n",
    "\n",
    "After retrieving these informations, we will use analysis of the data got from this process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
